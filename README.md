# ðŸ’¸ Data Finance Hub: Pipeline ELT End-to-End

Projeto feito para o acompanhamento automatizado do cÃ¢mbio. Este projeto utiliza uma abordagem Data Lakehouse para ingerir, armazenar e estruturar dados histÃ³ricos do DÃ³lar, permitindo o monitoramento de volatilidade e variaÃ§Ãµes de preÃ§o atravÃ©s de um fluxo ELT robusto.

## ðŸ— Arquitetura do Projeto

O pipeline segue o fluxo ELT (Extract, Load, Transform) orientado a eventos:
- **IngestÃ£o (Low-Code)**: O n8n consulta uma API pÃºblica de cotaÃ§Ãµes financeiras e deposita os dados brutos (JSON) no Data Lake.
- **Data Lake (Storage)**: O MinIO atua como Object Storage (compatÃ­vel com AWS S3), armazenando os arquivos na camada Raw.
- **OrquestraÃ§Ã£o**: O Apache Airflow utiliza um S3KeySensor para detectar a chegada de novos arquivos no bucket.
- **Processamento & Carga**: Uma DAG em Python processa o JSON (tratando listas/dicionÃ¡rios), valida os dados e insere as informaÃ§Ãµes estruturadas no Data Warehouse.
- Data **Warehouse**: O PostgreSQL armazena os dados finais prontos para anÃ¡lise.

## ðŸ›  Tech Stack

- **OrquestraÃ§Ã£o**: Apache Airflow 2.10.3 (Arquitetura Celery com Redis).
- **IngestÃ£o/AutomaÃ§Ã£o**: n8n.
- **Object Storage**: MinIO (Para simular AWS S3).
- **Banco de Dados**: PostgreSQL 13.
- **Infraestrutura**: Docker & Docker Compose.

## ðŸš€ Como Executar

### 1. PrÃ©-requisitos

Certifique-se de ter instalado:
- Docker Desktop & Docker Compose
- Git

### 2. InstalaÃ§Ã£o

- Clone o repositÃ³rio e configure as permissÃµes de usuÃ¡rio:
```bash
    git clone https://github.com/enzortorres/data-finance-hub.git
    cd data-finance-hub

    # Linux/Mac (Configura permissÃ£o do usuÃ¡rio Airflow)
    echo "AIRFLOW_UID=$(id -u)" > .env

    # Windows PowerShell (Configura permissÃ£o padrÃ£o)
    echo "AIRFLOW_UID=50000" > .env
```

- Suba o ambiente:
```bash
    docker compose up -d
```
- Aguarde alguns minutos na primeira execuÃ§Ã£o para que o Airflow realize as migraÃ§Ãµes do banco.

## âš™ï¸ ConfiguraÃ§Ã£o (PÃ³s-InstalaÃ§Ã£o)

### 1. Acesso Ã s Interfaces

|ServiÃ§o|URL|UsuÃ¡rio|Senha|
|:--|:--|:--|:--|
|Airflow|http://localhost:8080|admin|admin|
|MinIO|http://localhost:9001|minioadmin|minioadmin|
|n8n|http://localhost:5678|admin|admin|

### 2. Configurar Bucket (MinIO)

1. Acesse o MinIO (localhost:9001).
2. Crie um bucket chamado: ```raw-data```.

### 3. Configurar ConexÃµes no Airflow

- No menu Admin > Connections, crie/edite as seguintes conexÃµes:

1. ConexÃ£o Postgres (```postgres_dw```)
- **Conn Type**: ```Postgres```
- **Host**: ```postgres```
- **Schema**: ```airflow```
- **Login**: ```airflow```
- **Password**: ```airflow```
- **Port**: ```5432```

2. ConexÃ£o MinIO (```minio_conn```)
- **Conn Type**: ```Amazon Web Services```
- **AWS Access Key ID**: ```minioadmin```
- **AWS Secret Access Key**: ```minioadmin```
- **Extra**:
```json
    {
        "endpoint_url": "http://minio:9000"
    }
```
### 4. Configurar o workflow (```n8n```)

#### PreparaÃ§Ã£o

- Acesse o n8n: http://localhost:5678
- **UsuÃ¡rio**: admin
- **Senha**: admin
- Clique em "Add Workflow".

#### Passo 1: O Gatilho (Schedule Trigger) define a periodicidade da ingestÃ£o.

Adicione o nÃ³ Schedule Trigger.

- **Trigger Interval**: ```Hours```
- **Hours Between Triggers**: ```1``` (ou o intervalo que preferir para testes).

#### Passo 2: Buscar Dados (HTTP Request)

- Adicione o nÃ³ HTTP Request.
- **Method**: ```GET```
- **URL**: ```https://economia.awesomeapi.com.br/last/USD-BRL```
- **Authentication**: ```None```
- Clique em Execute Node para garantir que o JSON chegou.

ex: 
```json
    {
        "USDBRL": {...}
    }
```

#### Passo 3: Criar o Arquivo (Convert to File).

- Adicione o nÃ³ Convert to File.
- **Operation**: ```Convert to JSON```
- **Mode**: ```All items to One File```
- **Put Output File in Field**: ```data```

#### Passo 4: Configurar Credencial MinIO
Se ainda nÃ£o configurou:

- VÃ¡ em ```Credentials``` > ```Add Credential.```
- Escolha ```S3```.
- **Region**: ```us-east-1.```
- **Access Key ID**: ```minioadmin```
- **Secret Access Key**: ```minioadmin```
- **Endpoint**: ```http://minio:9000```
- **Force Path **Style: ative ON (Essencial).


#### Passo 5: Enviar para o Lake (S3 Node)
O passo final de carga.

Adicione o nÃ³ **S3** (o genÃ©rico/nativo).

- **Credential**: Selecione a credencial criada acima.
- **Operation**: ```Upload```
- **Bucket Name**: ```raw-data```

Para evitar problemas futuros com Windows por causa de espaÃ§os e dois pontos:

- **File Name**: Clique na engrenagem (Expression) e insira: ```cotacao-{{ $('HTTP Request').item.json.USDBRL.create_date.replace(' ', '_').replace(':', '-') }}.json```

Isso transforma ```2025-12-11 16:30:00``` em ```2025-12-11_16-30-00```.
- **Input Binary Field**: Garanta que estÃ¡ escrito ```data``` (ou o mesmo nome que vocÃª definiu no Passo 3).

## ðŸ§ª Testando o Pipeline

1.  No **n8n**, execute o workflow manualmente.
2.  Verifique no **MinIO** se o arquivo JSON apareceu no bucket `raw-data`.
3.  No **Airflow**, a DAG `data_etl_dolar` deve sair do estado de espera do sensor e processar o arquivo.
4.  Conecte-se ao banco via **DBeaver** (ou outro client SQL) para validar os dados:

    * **Host:** `localhost`
    * **Port:** `5432`
    * **Database:** `airflow`
    * **Username:** `airflow`
    * **Password:** `airflow`

    Execute a query:

```sql
    SELECT * FROM cotacao_dolar;
```