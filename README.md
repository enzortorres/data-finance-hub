# ðŸ’¸ Data Finance Hub: Pipeline ELT End-to-End

Projeto feito para o acompanhamento automatizado do cÃ¢mbio. Este projeto utiliza uma abordagem Data Lakehouse para ingerir, armazenar e estruturar dados histÃ³ricos do DÃ³lar, permitindo o monitoramento de volatilidade e variaÃ§Ãµes de preÃ§o atravÃ©s de um fluxo ELT robusto.

## ðŸ— Arquitetura do Projeto

O pipeline segue o fluxo ELT (Extract, Load, Transform) orientado a eventos:
- **IngestÃ£o (Low-Code)**: O n8n consulta uma API pÃºblica de cotaÃ§Ãµes financeiras e deposita os dados brutos (JSON) no Data Lake.
- **Data Lake (Storage)**: O MinIO atua como Object Storage (compatÃ­vel com AWS S3), armazenando os arquivos na camada Raw.
- **OrquestraÃ§Ã£o**: O Apache Airflow utiliza um S3KeySensor para detectar a chegada de novos arquivos no bucket.
- **Processamento & Carga**: Uma DAG em Python processa o JSON (tratando listas/dicionÃ¡rios), valida os dados e insere as informaÃ§Ãµes estruturadas no Data Warehouse.
- Data **Warehouse**: O PostgreSQL armazena os dados finais prontos para anÃ¡lise.

## ðŸ›  Tech Stack

- **OrquestraÃ§Ã£o**: Apache Airflow 2.10.3 (Arquitetura Celery com Redis).
- **IngestÃ£o/AutomaÃ§Ã£o**: n8n.
- **Object Storage**: MinIO (Para simular AWS S3).
- **Banco de Dados**: PostgreSQL 13.
- **Infraestrutura**: Docker & Docker Compose.

## ðŸš€ Como Executar

### 1. PrÃ©-requisitos

Certifique-se de ter instalado:
- Docker Desktop & Docker Compose
- Git

### 2. InstalaÃ§Ã£o

- Clone o repositÃ³rio e configure as permissÃµes de usuÃ¡rio:
```bash
    git clone https://github.com/enzortorres/data-finance-hub.git
    cd data-finance-hub

    # Linux/Mac (Configura permissÃ£o do usuÃ¡rio Airflow)
    echo "AIRFLOW_UID=$(id -u)" > .env

    # Windows PowerShell (Configura permissÃ£o padrÃ£o)
    echo "AIRFLOW_UID=50000" > .env
```

- Suba o ambiente:
```bash
    docker compose up -d
```
- Aguarde alguns minutos na primeira execuÃ§Ã£o para que o Airflow realize as migraÃ§Ãµes do banco.

## âš™ï¸ ConfiguraÃ§Ã£o (PÃ³s-InstalaÃ§Ã£o)

### 1. Acesso Ã s Interfaces

|ServiÃ§o|URL|UsuÃ¡rio|Senha|
|:--|:--|:--|:--|
|Airflow|http://localhost:8080|admin|admin|
|MinIO|http://localhost:9001|minioadmin|minioadmin|
|n8n|http://localhost:5678|admin|admin|

### 2. Configurar Bucket (MinIO)

1. Acesse o MinIO (localhost:9001).
2. Crie um bucket chamado: ```raw-data```.

### 3. Configurar ConexÃµes no Airflow

- No menu Admin > Connections, crie/edite as seguintes conexÃµes:

1. ConexÃ£o Postgres (```postgres_dw```)
- **Conn Type**: ```Postgres```
- **Host**: ```postgres```
- **Schema**: ```airflow```
- **Login**: ```airflow```
- **Password**: ```airflow```
- **Port**: ```5432```

2. ConexÃ£o MinIO (```minio_conn```)
- **Conn Type**: ```Amazon Web Services```
- **AWS Access Key ID**: ```minioadmin```
- **AWS Secret Access Key**: ```minioadmin```
- **Extra**:
```json
    {
        "endpoint_url": "http://minio:9000"
    }
```
### 4. Configurar o workflow (```n8n```)

#### PreparaÃ§Ã£o

- Acesse o n8n: http://localhost:5678
- **UsuÃ¡rio**: admin
- **Senha**: admin
- Clique em "Add Workflow".

#### Passo 1: O Gatilho (Schedule Trigger) define a periodicidade da ingestÃ£o.

Adicione o nÃ³ Schedule Trigger.

- **Trigger Interval**: ```Hours```
- **Hours Between Triggers**: ```1``` (ou o intervalo que preferir para testes).

#### Passo 2: Buscar Dados (HTTP Request)

- Adicione o nÃ³ HTTP Request.
- **Method**: ```GET```
- **URL**: ```https://economia.awesomeapi.com.br/last/USD-BRL```
- **Authentication**: ```None```
- Clique em Execute Node para garantir que o JSON chegou.

ex: 
```json
    {
        "USDBRL": {...}
    }
```

#### Passo 3: Criar o Arquivo (Convert to File).

- Adicione o nÃ³ Convert to File.
- **Operation**: ```Convert to JSON```
- **Mode**: ```All items to One File```
- **Put Output File in Field**: ```data```

#### Passo 4: Configurar Credencial MinIO
Se ainda nÃ£o configurou:

- VÃ¡ em ```Credentials``` > ```Add Credential.```
- Escolha ```S3```.
- **Region**: ```us-east-1.```
- **Access Key ID**: ```minioadmin```
- **Secret Access Key**: ```minioadmin```
- **Endpoint**: ```http://minio:9000```
- **Force Path Style**: ative ON (Essencial).


#### Passo 5: Enviar para o Lake (S3 Node)
O passo final de carga.

Adicione o nÃ³ **S3** (o genÃ©rico/nativo).

- **Credential**: Selecione a credencial criada acima.
- **Operation**: ```Upload```
- **Bucket Name**: ```raw-data```

Para evitar problemas futuros com Windows por causa de espaÃ§os e dois pontos:

- **File Name**: Clique na engrenagem (Expression) e insira: ```cotacao-{{ $('HTTP Request').item.json.USDBRL.create_date.replace(' ', '_').replace(':', '-') }}.json```

Isso transforma ```2025-12-11 16:30:00``` em ```2025-12-11_16-30-00```.
- **Input Binary Field**: Garanta que estÃ¡ escrito ```data``` (ou o mesmo nome que vocÃª definiu no Passo 3).

### ðŸ“¥Ou vocÃª pode importar o Workflow no n8n

1. Clique na seta abaixo para expandir o cÃ³digo.
2. Clique no Ã­cone de **Copiar** (ðŸ“‹) que aparece no canto superior direito do cÃ³digo.
3. No n8n, pressione `Ctrl+V` (ou `Cmd+V`) na tela do editor.

<details>
  <summary><strong>ðŸ“‹ Clique aqui para ver o JSON do Workflow</strong></summary>

```json
    {
    "nodes": [
        {
        "parameters": {
            "rule": {
            "interval": [
                {
                "field": "hours"
                }
            ]
            }
        },
        "type": "n8n-nodes-base.scheduleTrigger",
        "typeVersion": 1.3,
        "position": [
            0,
            0
        ],
        "id": "9f0b3c56-6ca9-48c4-be72-e447e6ce178e",
        "name": "Schedule Trigger"
        },
        {
        "parameters": {
            "url": "https://economia.awesomeapi.com.br/last/USD-BRL",
            "options": {
            "response": {
                "response": {
                "responseFormat": "json"
                }
            }
            }
        },
        "type": "n8n-nodes-base.httpRequest",
        "typeVersion": 4.3,
        "position": [
            208,
            0
        ],
        "id": "3f5e214c-decc-4910-9331-54649a15a180",
        "name": "HTTP Request"
        },
        {
        "parameters": {
            "operation": "upload",
            "bucketName": "raw-data",
            "fileName": "=cotacao-{{ $('HTTP Request').item.json.USDBRL.create_date.replace(' ', '_').replaceAll(':', '-') }}.json",
            "additionalFields": {}
        },
        "type": "n8n-nodes-base.s3",
        "typeVersion": 1,
        "position": [
            624,
            0
        ],
        "id": "e294f352-14ed-4a47-9d6c-4d13b9f60d3d",
        "name": "Upload a file",
        "credentials": {
            "s3": {
            "id": "ynRzp1T2055MQckQ",
            "name": "S3 account"
            }
        }
        },
        {
        "parameters": {
            "operation": "toJson",
            "options": {}
        },
        "type": "n8n-nodes-base.convertToFile",
        "typeVersion": 1.1,
        "position": [
            416,
            0
        ],
        "id": "09460834-e0a6-4493-a20e-c3472f549368",
        "name": "Convert to File"
        }
    ],
    "connections": {
        "Schedule Trigger": {
        "main": [
            [
            {
                "node": "HTTP Request",
                "type": "main",
                "index": 0
            }
            ]
        ]
        },
        "HTTP Request": {
        "main": [
            [
            {
                "node": "Convert to File",
                "type": "main",
                "index": 0
            }
            ]
        ]
        },
        "Convert to File": {
        "main": [
            [
            {
                "node": "Upload a file",
                "type": "main",
                "index": 0
            }
            ]
        ]
        }
    },
    "pinData": {},
    "meta": {
        "templateCredsSetupCompleted": true,
        "instanceId": "6271dbfb12293172553a17d793f5476298b06e742635a6df2e07990c24336f6a"
    }
    }
```
</details>

## ðŸ§ª Testando o Pipeline

1.  No **n8n**, execute o workflow manualmente.
2.  Verifique no **MinIO** se o arquivo JSON apareceu no bucket `raw-data`.
3.  No **Airflow**, a DAG `data_etl_dolar` deve sair do estado de espera do sensor e processar o arquivo.
4.  Conecte-se ao banco via **DBeaver** (ou outro client SQL) para validar os dados:

    * **Host:** `localhost`
    * **Port:** `5432`
    * **Database:** `airflow`
    * **Username:** `airflow`
    * **Password:** `airflow`

    Execute a query:

```sql
    SELECT * FROM cotacao_dolar;
```